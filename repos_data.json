[
  {
    "name": "A-Review-Paper-on-Toxic-Comment-Classification-",
    "readme": "# Toxic Comment Classification\n\nThis paper focuses on the automatic classification of toxic comments using machine learning and natural language processing techniques. It addresses the growing challenge of hate speech, harassment, and inappropriate content on online platforms, aiming to promote safer digital interactions.\n\n## Features\n- **Machine Learning Models**: Logistic Regression, SVM, Decision Trees, and advanced deep learning models like LSTM and BERT.\n- **Data Imbalance Handling**: Oversampling, undersampling, and synthetic data generation (SMOTE).\n- **Explainability**: Utilizes techniques like LIME and SHAP for model interpretability.\n- **Dataset Support**: Works with widely-used datasets like Jigsaw, Wikipedia Comments, and Kaggle Toxic Comment datasets.\n\n## Goals\n- Enhance online moderation with automated tools.\n- Improve safety and foster respectful interactions in digital spaces.\n- Address challenges like language complexity, data imbalance, and contextual sensitivity.\n\n## Contributions\nThis research provides insights into the effectiveness of machine learning and NLP models for toxic comment classification, paving the way for robust, transparent, and context-sensitive moderation systems.\n\n## Authors\n- **Trisha Sharma**  \n- **Anahita Bhandari**  \n\n## License\nThis project is licensed under the [MIT License](LICENSE).\n\n## Acknowledgments\n- Datasets: Jigsaw, Wikipedia Comments, Kaggle Toxic Comment datasets.\n- References: Research works on toxic comment classification and NLP techniques.\n",
    "topics": [],
    "language": null
  },
  {
    "name": "Adidas-Sales-Dashboard-in-Power-BI",
    "readme": "# Adidas Sales Dashboard in Power BI\n\n## Overview\nThis project involves designing and developing an interactive Power BI dashboard to analyze Adidas sales data. The dashboard provides insights into sales trends, product performance, regional distribution, and key performance indicators (KPIs) to support data-driven decision-making.\n\n## Features\n- **Comprehensive Analysis**: Visualizes sales data for revenue, product categories, and regional performance.\n- **Actionable Insights**: Identifies growth opportunities and tracks KPIs like operating margins and total sales.\n- **Data Highlights**:\n  - Analyzed over **10,000 data points**.\n  - Uncovered **15% revenue growth opportunities**.\n  - Monitored **$2.4M in total sales** across multiple product categories.\n\n## Technologies Used\n- **Power BI**: For dashboard creation and data visualization.\n- **Excel**: For initial data storage and preprocessing.\n\n## Report Image\n![Adidas Sales Dashboard](adidas.png)\n\n## Dataset\nThe dataset includes information on:\n- Retailers\n- Invoice dates\n- Regions, states, and cities\n- Product categories\n- Sales figures (units sold, total sales, operating profit, etc.)\n\n## How to Use\n1. Open the Power BI file: `adidasSalereport.pbix`.\n2. Interact with the visualizations to explore sales insights.\n3. Modify or update the dataset to analyze new trends.\n\n## Key Insights\n- Identified regions with the highest revenue contributions.\n- Evaluated product performance to optimize inventory management.\n- Pinpointed key areas for improving operational efficiency.\n",
    "topics": [],
    "language": null
  },
  {
    "name": "AI-Agentic-System",
    "readme": "# **AI-Powered Research & Answering System using LangGraph, LangChain, Tavily, and Hugging Face**\n\n## **Overview**\nThis project builds an **AI-powered research assistant** that:\n1. **Searches the web** using **Tavily API** to gather relevant information.\n2. **Processes the results** and structures an insightful response.\n3. **Generates a well-formatted answer** using **Hugging Face‚Äôs Falcon-7B** model.\n4. **Manages workflow** with **LangGraph**, ensuring smooth execution from query to answer.\n5. **Leverages LangChain** for potential future **retrieval-augmented generation (RAG)** and **LLM-powered research workflows**.\n\n---\n\n## **Features**\n‚úÖ **Automated Web Research**: Uses **Tavily API** for real-time, relevant search results.  \n‚úÖ **AI-Powered Answer Generation**: Utilizes **Hugging Face Falcon-7B** for response synthesis.  \n‚úÖ **LangGraph Workflow Management**: Structured **stateful execution** for consistent results.  \n‚úÖ **LangChain Integration**: Future-ready for **document retrieval**, **memory**, and **agent-based reasoning**.  \n‚úÖ **Secure API Handling**: Uses **dotenv** for managing API keys securely.  \n\n---\n\n## **Technologies Used**\n- **Python**\n- **LangGraph** (for structured research workflows)\n- **LangChain** (for advanced RAG and memory-based interactions)\n- **Tavily API** (for real-time web searches)\n- **Hugging Face API** (for LLM-powered text generation)\n- **Dotenv** (for environment variable management)\n\n---\n\n## **Setup Instructions**\n\n### **1Ô∏è‚É£ Clone the Repository**\n```sh\ngit clone https://github.com/your-repo-name.git\ncd your-repo-name\n```\n\n### **2Ô∏è‚É£ Install Dependencies**\nEnsure you have Python installed (`>=3.8`). Then, install required libraries:\n```sh\npip install langgraph langchain tavily-python huggingface_hub python-dotenv requests\n```\n\n### **3Ô∏è‚É£ Configure API Keys**\nCreate a `.env` file in the root directory and add your **Tavily** and **Hugging Face** API keys:\n```sh\nTAVILY_API_KEY=your_tavily_api_key_here\nHUGGINGFACEHUB_API_KEY=your_huggingface_api_key_here\n```\n‚ö†Ô∏è **Do not share API keys**. Add `.env` to `.gitignore` before committing.\n\n### **4Ô∏è‚É£ Run the System**\nExecute the script:\n```sh\npython main.py\n```\nYou will be prompted to enter a query:\n```\nEnter your research query: <your-question-here>\n```\nThe system will **search**, **generate an AI response**, and **display the final answer**.\n\n---\n\n## **How It Works**\n1. **User enters a query** (e.g., *\"What are the latest AI trends?\"*).\n2. **Tavily fetches search results** (top 5 web pages).\n3. **LangGraph manages workflow execution**.\n4. **Data is passed to Hugging Face Falcon-7B** for response generation.\n5. **Final Answer is displayed**.\n\n---\n\n## **LangGraph vs. LangChain**\n| Feature | LangGraph | LangChain |\n|---------|----------|-----------|\n| **Primary Use** | Workflow automation for AI agents | LLM-powered reasoning & memory |\n| **Execution Model** | Graph-based state transitions | Chain-of-thought & agents |\n| **Ideal For** | Structured research, data pipelines | Conversational AI, RAG |\n| **Integration** | Can be used **with** LangChain | Can use LangGraph for structured execution |\n\nThis project **uses LangGraph** for structured workflow execution but can be extended **with LangChain** for **memory** and **retrieval-augmented generation (RAG)**.\n\n---\n\n## **Code Breakdown**\n### **Research Workflow**\n- **`research_agent(state: ResearchState)`**: Fetches data using **Tavily API**.\n- **`answer_drafting_agent(state: ResearchState)`**: Generates a structured answer using **Hugging Face Falcon-7B**.\n- **LangGraph manages execution**, ensuring smooth transitions.\n\n### **Core Data Model**\n```python\n@dataclass\nclass ResearchState:\n    query: str\n    research_data: list\n    answer_draft: str\n```\n- **`query`**: User input.\n- **`research_data`**: Web search results.\n- **`answer_draft`**: AI-generated response.\n\n---\n## **Future Enhancements**\nüîπ **Integrate LangChain‚Äôs Retrieval-Augmented Generation (RAG)** for better long-term research.  \nüîπ **Use NVIDIA DeepSeek-R1** for better AI-generated insights.  \nüîπ **Implement a UI** using Streamlit or Flask for a user-friendly interface.  \n\n---\n\n## **Example Output**\n![image](https://github.com/user-attachments/assets/856778dd-b8fe-4938-bbd0-058aa55396cf)\n![image](https://github.com/user-attachments/assets/eaec0f6e-43af-454e-975d-c85590bc203a)\n\n---\n\n## **Contributing**\nPull requests are welcome!  \nFor major changes, please open an issue first to discuss your ideas.  \n\n---\n\n## **License**\nMIT License - Feel free to use and modify this project.\n  \n\n\n\n\n\n\n\n\n\n",
    "topics": [],
    "language": "Python"
  },
  {
    "name": "Animal-info-provider-chatbot",
    "readme": "![image](https://github.com/user-attachments/assets/ad56f9d2-6c35-4149-87d5-95b2a20b6ef6)\n\n## THIS README IS NOT FIXED AND WILL BE CHANGED AFTER THE PROJECT COMPLETION\n## **Project Overview**\nThis project is a Flask-based application designed to integrate multiple components for handling imagery data, classifying animals, and providing detailed information about them through a custom QnA chatbot. The system uses MongoDB for storage, OpenCV for image classification, and a custom RAG (Retrieval-Augmented Generation) model for answering questions about animals based on a custom dataset.\n\n---\n\n## **Features**\n1. **Image Classification:**\n   - Users can upload images of animals.\n   - OpenCV processes the image, and a machine learning model classifies the animal.\n\n2. **Custom QnA Chatbot:**\n   - Uses a custom-trained RAG model to answer user queries about animals.\n   - Knowledge base includes books, datasets, and additional resources.\n\n3. **MongoDB Integration:**\n   - Imagery data and metadata are stored in MongoDB for efficient retrieval.\n\n4. **Frontend Integration:**\n   - Flask connects to a ReactJS/HTML-based frontend for user interaction.\n   - Users can upload images and chat with the chatbot through a single interface.\n\n5. **Modular Design:**\n   - Supports multiple chatbots integrated into a single platform.\n\n---\n\n## **Architecture**\n1. **Frontend:**\n   - ReactJS/HTML for user interface.\n   - Features include image upload and chatbot interaction.\n\n2. **Backend (Flask):**\n   - REST API endpoints for:\n     - Image upload and classification.\n     - Chatbot communication.\n   - Modular architecture to manage multiple chatbots.\n\n3. **Image Processing:**\n   - OpenCV used for preprocessing and classification.\n   - Model trained on labeled animal datasets using TensorFlow/PyTorch.\n\n4. **Chatbot (RAG Model):**\n   - Retrieval-Augmented Generation model trained on a custom animal dataset.\n   - Combines a retriever for finding relevant knowledge and a generator for answering queries.\n\n5. **Database:**\n   - MongoDB for storing:\n     - Images and metadata.\n     - User queries and session history.\n## **Architecture Diagram**\n![SWOC_Project_Diagram drawio](https://github.com/user-attachments/assets/6180a546-f986-4513-ada4-328dda5d01e3)\n\n---\n\n## **Installation**\n### **Prerequisites**\n- Python 3.8+\n- MongoDB\n- Node.js (for frontend)\n- OpenCV\n- TensorFlow/PyTorch\n- Flask\n\n### **Steps**\n1. **Clone the Repository:**\n   ```bash\n   git clone https://github.com/username/animal-classification-chatbot.git\n   cd animal-classification-chatbot\n   ```\n\n2. **Set Up Backend:**\n   - Install Python dependencies:\n     ```bash\n     pip install -r requirements.txt\n     ```\n   - Run the Flask server:\n     ```bash\n     python app.py\n     ```\n\n3. **Set Up Frontend:**\n   - Navigate to the frontend directory:\n     ```bash\n     cd frontend\n     ```\n   - Install dependencies and start the frontend:\n     ```bash\n     npm install\n     npm start\n     ```\n\n4. **Run MongoDB:**\n   - Ensure MongoDB is running locally or use a cloud-based MongoDB instance.\n\n---\n\n## **Usage**\n1. **Upload Image:**\n   - Navigate to the frontend.\n   - Upload an animal image.\n   - The backend processes the image and returns the classification result.\n\n2. **Interact with Chatbot:**\n   - Ask questions about animals (e.g., \"Tell me about lions\").\n   - The chatbot fetches relevant information from the custom knowledge base and provides detailed answers.\n\n---\n\n## **Endpoints**\n### **Image Classification**\n- **POST** `/classify`\n  - Input: Image file\n  - Output: Classified animal name and confidence score.\n\n### **Chatbot**\n- **POST** `/chatbot`\n  - Input: User query\n  - Output: Chatbot response.\n\n---\n\n## **Tech Stack**\n- **Frontend:** ReactJS, HTML, CSS\n- **Backend:** Flask, OpenCV, TensorFlow/PyTorch\n- **Database:** MongoDB\n- **Chatbot:** Custom RAG model\n\n---\n\n## **Future Enhancements**\n1. Add live camera feed processing for real-time animal classification.\n2. Enhance chatbot with multilingual support.\n3. Deploy the system on cloud platforms like AWS/GCP for scalability.\n4. Integrate user authentication for personalized experiences.\n\n---\n\n## **Contributing**\nContributions are welcome! Please fork the repository and submit a pull request.\n\n---\n\n## **License**\nThis project is licensed under the MIT License. See `LICENSE` for more details.\n\n---\n\nKuch aur customize karna ho toh bolna, bhai! üòä\n",
    "topics": [],
    "language": null
  },
  {
    "name": "Austrailian-Airport-Footfall-Dahsboard",
    "readme": "# Australian Airports Dashboard - Power BI\n\n![Power BI Dashboard](image.png) \n\n\nThis repository contains a **Power BI dashboard** that comprehensively overviews all Australian airports. The dashboard visualizes key metrics, trends, and insights related to airport operations, passenger traffic, and other relevant data.\n\n---\n\n## Table of Contents\n1. [Overview](#overview)\n2. [Features](#features)\n3. [Data Sources](#data-sources)\n4. [How to Use](#how-to-use)\n5. [Contributing](#contributing)\n6. [License](#license)\n\n---\n\n## Overview\nThe Australian Airports Dashboard is a **data visualization tool** built using Microsoft Power BI. It provides an interactive and user-friendly interface to explore data related to Australian airports. The dashboard includes metrics such as passenger numbers, flight statistics, airport locations, and more.\n\n---\n\n## Features\n- **Interactive Visualizations**: Explore data using interactive charts, maps, and tables.\n- **Key Metrics**: View passenger traffic, flight delays, and airport rankings.\n- **Filters and Slicers**: Customize the view by filtering data based on airport, state, or time period.\n- \n## Data Sources\nThe data used in this dashboard is sourced from publicly available datasets, including:\n- **Australian Bureau of Statistics (ABS)**\n- **Australian Government Department of Infrastructure, Transport, Regional Development, and Communications**\n- **OpenStreetMap** for geospatial data\n\n---\n\n## How to Use\n1. **Download the Power BI File**: Clone this repository or download the `.pbix` file.\n2. **Open in Power BI**: Open the file using Microsoft Power BI Desktop.\n3. **Explore the Dashboard**: Use the interactive filters and slicers to explore the data.\n4. **Refresh Data**: If you have access to the data sources, you can refresh the data in Power BI to get the latest updates.\n\n---\n\n## Contributing\nContributions are welcome! If you'd like to improve this dashboard or add new features, please follow these steps:\n1. Fork the repository.\n2. Create a new branch (`git checkout -b feature/YourFeatureName`).\n3. Commit your changes (`git commit -m 'Add some feature'`).\n4. Push to the branch (`git push origin feature/YourFeatureName`).\n5. Open a pull request.\n\n---\n\n## License\nThis project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.\n\n---\n**Happy Analyzing!** üöÄ\n",
    "topics": [],
    "language": null
  },
  {
    "name": "Book-Rating-predictions",
    "readme": "# Book Rating Prediction\n\nA project to predict **Book-Ratings** using user-book interaction data, simulating real-world recommendation systems.\n\n## Objectives\n- Analyze rating distributions and user/book activity.\n- Preprocess data: handle missing values, encode features, address sparsity.\n- Build models: Matrix Factorization, Collaborative Filtering, Neural Networks.\n- Evaluate using RMSE and MAE.\n- Optimize: tune hyperparameters and handle class imbalances.\n\n## Tech Stack\n- **Python**, **Pandas**, **NumPy**\n- **Seaborn**, **Matplotlib**\n- **Surprise**, **TensorFlow/Keras**\n- **Google Colab**\n\n## Usage\n1. **Run on Google Colab**:\n   - Clone the repo:\n     ```bash\n     git clone https://github.com/your-repo/book-recommendation-system.git\n     ```\n   - Upload the notebook to Google Drive and open it in Colab.\n\n2. **Contribute**:\n   - Fork the repo, create a feature branch, make changes, and submit a pull request.\n\n## License\nLicensed under the [MIT License](LICENSE).\n\n## STATS\n\n### Rating Statistics:\n- **Strong zero-inflation**: Many books have 0 ratings.\n- **count**: 1.149780e+06\n- **mean**: 2.866950e+00\n- **std**: 3.854184e+00\n- **min**: 0.000000e+00\n\n### User Activity Statistics:\n- **Average ratings per user**: 10.92 times\n- **Median ratings per user**: 1.0\n- **Max ratings by a single user**: 13602 times\n\n### Book Activity Statistics:\n- **Average ratings per book**: 3.38 times\n- **Median ratings per book**: 1.0\n- **Max ratings for a single book**: 2502 times\n\n### Data Preprocessing:\n- **No missing values**.\n- **Encoded ISBNs to numerical values**.\n- **Filtered users with 5+ ratings** and books with 5+ ratings.\n- **Normalized ratings** to 0-1 scale.\n- **Reduced dataset from 1M to 541K ratings** while maintaining quality interactions.\n\n### Model 1: Collaborative Filtering using SVD (Singular Value Decomposition)\n- **Reason for choosing SVD**: It's a widely used matrix factorization technique that effectively handles sparse datasets, capturing latent factors for users and items to predict ratings accurately.\n- **Performance**:\n  - **RMSE**: 3.54\n  - **MAE**: 2.81\n- **SVD Errors Observations**:\n  - **Peak at 0 Error**: Many near-perfect predictions, which is a good sign.\n  - **Negative Skew (Overestimation)**: The model tends to over-predict more often, as observed in the range of -5 to -2.5.\n  - **Positive Errors (Underestimation)**: Some significant underestimation between 5 and 10.\n  \n### After Tuning SVD:\n1. **Clustering Along Vertical Lines**:\n   - The model struggles to differentiate among books with similar features, indicating that further adjustments are needed.\n   - After tuning, the clustering is less pronounced, but there is still room for improvement.\n   \n2. **Scattering Around the Red Line**:\n   - Points scatter around the red line (perfect predictions), indicating prediction errors. A tighter alignment would show higher accuracy, reflecting the effect of optimized hyperparameters.\n\n---\n\n### Model 2: Neural Collaborative Filtering (NCF)\n- **Why NCF?**:\n  - NCF leverages neural networks to learn complex, non-linear relationships between users and items, making it more accurate and personalized than traditional methods.\n  - NCF captures intricate user preferences and can predict ratings that traditional collaborative filtering methods struggle with.\n  \n- **Performance**:\n  - **Accuracy (Exact Match)**: 16.95%\n  - **RMSE** and **MAE** for NCF were also calculated.\n  - **Challenges**:\n    - Exact match accuracy can be low due to the nature of continuous rating scales (1-5).\n    - Recommendation systems generally focus on minimizing errors (RMSE, MSE) or ranking metrics like Precision@K and Recall@K.\n  \n---\n\n### Comparison of SVD and NCF\n\n| Model     | RMSE   | MAE   | Accuracy (Exact Match) |\n|-----------|--------|-------|------------------------|\n| **SVD**   | 3.54   | 2.81  | N/A                    |\n| **NCF**   | 3.91   | 2.95  | 16.95%                 |\n\n- **SVD**: Performs reasonably well with moderate accuracy but is limited by its linear nature in capturing user-item interactions.\n- **NCF**: Shows potential for personalized recommendations but has low exact match accuracy. This is common in recommendation systems, where minimizing prediction error is usually prioritized over exact accuracy.\n\n### Conclusion\n- Both SVD and NCF have their strengths and weaknesses.\n- **SVD** is fast and interpretable, but struggles with capturing complex, non-linear user-item interactions.\n- **NCF** can handle these complexities, but exact match accuracy may not always be high due to the continuous nature of ratings.\n- **Future Steps**: Consider further tuning of the NCF model, adding additional features, and exploring other recommendation system techniques like **Factorization Machines** or **Deep Learning-based models**.\n",
    "topics": [
      "machine-learning",
      "neural-network",
      "prediction-algorithm"
    ],
    "language": "Jupyter Notebook"
  },
  {
    "name": "Calculator",
    "readme": "# Java GUI Calculator\n\n\n## Introduction\nThis project is a simple Java calculator with a graphical user interface (GUI). The purpose of this project was to learn and practice creating GUI applications using Java.\n\n## Features\nBasic arithmetic operations: addition, subtraction, multiplication, and division.\nInput through button clicks on the GUI.\nDisplay of results on the GUI.\n\n## Prerequisites\nJava Development Kit (JDK) installed on your system.\nEclipse IDE (or any Java IDE of your choice) for development.\n\n## Installation\nClone or download this repository to your local machine.\nImport the project into your Java IDE. I used Eclipse IDE.\n\n## Usage\nRun the GUICalculator.java file to launch the calculator application.\nUse the buttons on the GUI to input numbers and perform calculations.\nThe result will be displayed on the GUI.\n\n## Development\nThe calculator GUI is created using Java Swing components.\nThe calculator logic is implemented in the Calculator.java class.\n\n## Result\n![Calculator](\"C:\\Users\\dell\\Downloads\\calc.jpg\")\n\n\n## Acknowledgements\nThis project was created as a learning exercise based on tutorials and documentation available online.\n\n\n\n",
    "topics": [
      "java"
    ],
    "language": "Java"
  },
  {
    "name": "Capstone-project-25",
    "readme": "# Capstone-project-25\nCapstone project leveraging Generative AI for automated video monitoring, intelligent summarization, and behavioral assessment ‚Äî by Team Data Mavericks\n### API Key Setup (Required Before Running)\n1. Create a `.env` file in the project root directory\n2. Add this line inside:GROQ_API_KEY=your_actual_key_here\n3. Save and run the Streamlit app.",
    "topics": [],
    "language": "Python"
  },
  {
    "name": "Churn-analysis-simulation",
    "readme": "# Churn Analysis Simulation\n=====================\n\nA customer churn analysis simulation project for XYZ Analytics, showcasing advanced data analytics skills using Python, Pandas, and NumPy.\n\n## About\n--------\n* .ipynb files contain all the Python codes for analysis and data evaluation.\n* .csv files contain all the datasets\n\nConducted by Trisha Sharma\n\n## Getting Started\n-------------------\n\n### Prerequisites\n\n* Python 3.x\n* Pandas\n* NumPy\n* Seaborn\n* Matplotlib\n* RandomForest\n* Jupyter Notebook\n\n### Installation\n\n* Install the required libraries using `pip`\n* Clone the repository to your local machine\n\n### Usage\n\n* Run each cell in the `.ipynb` files using the Jupyter Notebook extension in VSCode\n\n## Contributing\n--------------\n\nContributions are welcome! If you'd like to contribute to the project, please fork the repository and submit a pull request.\n\n## License\n---------\n\nThis project is released under the MIT License.\n\n## Workflow\n----------------\n\n* Completed a customer churn analysis simulation for XYZ Analytics, demonstrating advanced data analytics skills, identifying essential client data and outlining a strategic investigation approach.\n* Conducted efficient data analysis using Python, including Pandas and NumPy. Employed data visualization techniques for insightful trend interpretation.\n* Completed the engineering and optimization of a random forest model, achieving an 85% accuracy rate in predicting customer churn.\n* Completed a concise executive summary for the Associate Director, delivering actionable insights for informed decision-making based on the analysis.\n",
    "topics": [],
    "language": "Jupyter Notebook"
  },
  {
    "name": "Covid-19",
    "readme": "# COVID-19 Detection Using CNN Through Chest X-Ray Images\n\n## Overview\nThis repository contains the implementation of a Convolutional Neural Network (CNN) model to detect COVID-19 using chest X-ray images. The project demonstrates how deep learning techniques can assist in identifying COVID-19 cases with high accuracy.\n\n## Purpose\nThis project was a group assignment for our *Machine Learning* subject. It highlights the application of CNNs in medical image analysis and aims to provide a foundation for further research and improvements in AI-based diagnostic tools.\n\n## Team Members\n- *Trisha Sharma* (229309215)\n- *Anahita Bhandari* (229309186)\n\n## Dataset\nThe model was trained and tested on a dataset containing chest X-ray images categorized into COVID-19 positive, normal, and pneumonia cases. The dataset was preprocessed to ensure compatibility with the CNN model.\n\n## Methodology\n1. *Data Preprocessing*: \n   - Resized images for uniformity.\n   - Normalized pixel values for faster convergence.\n   - Split the data into training, validation, and test sets.\n\n2. *Model Architecture*:\n   - Built a CNN with multiple convolutional and pooling layers.\n   - Used ReLU activation and dropout for better generalization.\n   - Added fully connected layers to classify images.\n\n3. *Training*:\n   - Optimized the model using the Adam optimizer.\n   - Categorical cross-entropy as the loss function.\n   - Evaluated model performance using accuracy and loss metrics.\n\n4. *Testing*:\n   - Assessed the model on unseen test data.\n   - Generated a confusion matrix to analyze classification results.\n\n## Results\n- Achieved a high level of accuracy in classifying COVID-19 cases.\n- The model demonstrated the potential of CNNs in medical diagnostics.\n\n## How to Run the Project\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/<your-username>/covid19-detection-cnn.git\n2. Install Requirements:\n   pip install -r requirements.txt\n- Atlast train and test the model\n\n## Technologies Used\n- *Python*\n- *TensorFlow/Keras*\n- *OpenCV*\n- *NumPy, **pandas*\n- *Matplotlib* for visualization\n\n## Acknowledgments\nThis project was submitted as part of the coursework for our *Machine Learning* subject.  \nWe extend our gratitude to our faculty and peers for their guidance¬†and¬†support.\n",
    "topics": [
      "css",
      "html",
      "javascript",
      "jupyter-notebook",
      "python"
    ],
    "language": "JavaScript"
  },
  {
    "name": "deshboard-sidebar",
    "readme": "# Getting Started with Create React App\n\nThis project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).\n\n## Available Scripts\n\nIn the project directory, you can run:\n\n### `npm start`\n\nRuns the app in the development mode.\\\nOpen [http://localhost:3000](http://localhost:3000) to view it in your browser.\n\nThe page will reload when you make changes.\\\nYou may also see any lint errors in the console.\n\n### `npm test`\n\nLaunches the test runner in the interactive watch mode.\\\nSee the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.\n\n### `npm run build`\n\nBuilds the app for production to the `build` folder.\\\nIt correctly bundles React in production mode and optimizes the build for the best performance.\n\nThe build is minified and the filenames include the hashes.\\\nYour app is ready to be deployed!\n\nSee the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.\n\n### `npm run eject`\n\n**Note: this is a one-way operation. Once you `eject`, you can't go back!**\n\nIf you aren't satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.\n\nInstead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you're on your own.\n\nYou don't have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn't feel obligated to use this feature. However we understand that this tool wouldn't be useful if you couldn't customize it when you are ready for it.\n\n## Learn More\n\nYou can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).\n\nTo learn React, check out the [React documentation](https://reactjs.org/).\n\n### Code Splitting\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/code-splitting](https://facebook.github.io/create-react-app/docs/code-splitting)\n\n### Analyzing the Bundle Size\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size](https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size)\n\n### Making a Progressive Web App\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app](https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app)\n\n### Advanced Configuration\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/advanced-configuration](https://facebook.github.io/create-react-app/docs/advanced-configuration)\n\n### Deployment\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/deployment](https://facebook.github.io/create-react-app/docs/deployment)\n\n### `npm run build` fails to minify\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify](https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify)\n",
    "topics": [],
    "language": null
  },
  {
    "name": "designStudio",
    "readme": "",
    "topics": [],
    "language": null
  },
  {
    "name": "Face-Detection",
    "readme": "# Face Detection  <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/smahesh29/Gender-and-Age-Detection\">\n\n\n<h2>Objective :</h2>\n<p>To build a gender and age detector that can approximately guess the gender and age of the person (face) in a picture or through webcam.</p>\n\n<h2>About the Project :</h2>\n<p>In this Python Project, I had used Deep Learning to accurately identify the gender and age of a person from a single image of a face. I used the models trained by <a href=\"https://talhassner.github.io/home/projects/Adience/Adience-data.html\">Tal Hassner and Gil Levi</a>. The predicted gender may be one of ‚ÄòMale‚Äô and ‚ÄòFemale‚Äô, and the predicted age may be one of the following ranges- (0 ‚Äì 2), (4 ‚Äì 6), (8 ‚Äì 12), (15 ‚Äì 20), (25 ‚Äì 32), (38 ‚Äì 43), (48 ‚Äì 53), (60 ‚Äì 100) (8 nodes in the final softmax layer). It is very difficult to accurately guess an exact age from a single image because of factors like makeup, lighting, obstructions, and facial expressions. And so, I made this a classification problem instead of making it one of regression.</p>\n\n<h2>Dataset :</h2>\n<p>For this python project, I had used the Adience dataset; the dataset is available in the public domain and you can find it <a href=\"https://www.kaggle.com/ttungl/adience-benchmark-gender-and-age-classification\">here</a>. This dataset serves as a benchmark for face photos and is inclusive of various real-world imaging conditions like noise, lighting, pose, and appearance. The images have been collected from Flickr albums and distributed under the Creative Commons (CC) license. It has a total of 26,580 photos of 2,284 subjects in eight age ranges (as mentioned above) and is about 1GB in size. The models I used had been trained on this dataset.</p>\n\n<h2>Additional Python Libraries Required :</h2>\n<ul>\n  <li>OpenCV</li>\n  \n       pip install opencv-python\n</ul>\n<ul>\n <li>argparse</li>\n  \n       pip install argparse\n</ul>\n\n<h2>The contents of this Project :</h2>\n<ul>\n  <li>opencv_face_detector.pbtxt</li>\n  <li>opencv_face_detector_uint8.pb</li>\n  <li>age_deploy.prototxt</li>\n  <li>age_net.caffemodel</li>\n  <li>gender_deploy.prototxt</li>\n  <li>gender_net.caffemodel</li>\n  <li>a few pictures to try the project on</li>\n  <li>detect.py</li>\n </ul>\n <p>For face detection, we have a .pb file- this is a protobuf file (protocol buffer); it holds the graph definition and the trained weights of the model. We can use this to run the trained model. And while a .pb file holds the protobuf in binary format, one with the .pbtxt extension holds it in text format. These are TensorFlow files. For age and gender, the .prototxt files describe the network configuration and the .caffemodel file defines the internal states of the parameters of the layers.</p>\n \n <h2>Usage :</h2>\n <ul>\n  <li>Download my Repository</li>\n  <li>Open your Command Prompt or Terminal and change directory to the folder where all the files are present.</li>\n  <li><b>Detecting Gender and Age of face in Image</b> Use Command :</li>\n  \n      python detect.py --image <image_name>\n</ul>\n  <p><b>Note: </b>The Image should be present in same folder where all the files are present</p> \n<ul>\n  <li><b>Detecting Gender and Age of face through webcam</b> Use Command :</li>\n  \n      python detect.py\n</ul>\n<ul>\n  <li>Press <b>Ctrl + C</b> to stop the program execution.</li>\n</ul>\n\n# Working:\n[![Watch the video](https://img.youtube.com/vi/ReeccRD21EU/0.jpg)](https://youtu.be/ReeccRD21EU)\n\n<h2>Examples :</h2>\n<p><b>NOTE:- I downloaded the images from Google,if you have any query or problem i can remove them, i just used it for Educational purpose.</b></p>\n\n    >python detect.py --image girl1.jpg\n    Gender: Female\n    Age: 25-32 years\n    \n<img src=\"Example/Detecting age and gender girl1.png\">\n\n    >python detect.py --image girl2.jpg\n    Gender: Female\n    Age: 8-12 years\n    \n<img src=\"Example/Detecting age and gender girl2.png\">\n\n    >python detect.py --image kid1.jpg\n    Gender: Male\n    Age: 4-6 years    \n    \n<img src=\"Example/Detecting age and gender kid1.png\">\n\n    >python detect.py --image kid2.jpg\n    Gender: Female\n    Age: 4-6 years  \n    \n<img src=\"Example/Detecting age and gender kid2.png\">\n\n    >python detect.py --image man1.jpg\n    Gender: Male\n    Age: 38-43 years\n    \n<img src=\"Example/Detecting age and gender man1.png\">\n\n    >python detect.py --image man2.jpg\n    Gender: Male\n    Age: 25-32 years\n    \n<img src=\"Example/Detecting age and gender man2.png\">\n\n    >python detect.py --image woman1.jpg\n    Gender: Female\n    Age: 38-43 years\n    \n<img src=\"Example/Detecting age and gender woman1.png\">\n              \n",
    "topics": [],
    "language": "Python"
  },
  {
    "name": "Face-Recognition-Attendance-Projects",
    "readme": "# DEEP LEARNING LAB (0021)\r\n\r\n## Problem Statement\r\n\r\n**Face Detection System for Identifying Hostellers and Non-Hostellers with Name and Registration Number Classification.**\r\n\r\n### TEAM:\r\n\r\n1. Trisha Sharma 229309215\r\n2. Anahita Bhandari 229309186\r\n\r\n---\r\n\r\n## Project Document: Face Detection and Recognition System for Classroom Students\r\n\r\n### Objective\r\n\r\nThe primary objective of this project is to develop a face detection and recognition system that identifies students from a class of 20 individuals. The system will classify students based on their name and registration number, providing an efficient and automated method for managing attendance and monitoring student presence.\r\n\r\n---\r\n\r\n### Scope\r\n\r\n1. **Face Detection:** Utilize advanced algorithms to detect student faces in various settings with high accuracy.\r\n2. **Identification and Classification:** Implement facial recognition technology to identify and classify students based on their name and registration number.\r\n3. **Attendance Management:** Automate the process of logging student attendance and monitoring student presence in real-time.\r\n4. **Database Integration:** Maintain a database to store facial data, names, and registration numbers for real-time and future use.\r\n\r\n---\r\n\r\n### Methodology\r\n\r\n1. **Data Collection:**\r\n    - Collect facial images of all 20 students in diverse conditions (lighting, angles, etc.) to build a robust training dataset.\r\n    - Ensure the dataset is balanced and representative of the various scenarios the system might encounter.\r\n  \r\n2. **Model Development:**\r\n    - **Convolutional Neural Networks (CNNs):** Utilize CNNs for face detection and recognition tasks.\r\n    - **Pre-trained Models:** Leverage pre-trained deep learning models (e.g., **FaceNet**, **VGGFace**, or **ResNet**) to enhance the system‚Äôs accuracy and reduce training time.\r\n    - **OpenCV and Dlib:** Use OpenCV for real-time image processing and Dlib for face detection and feature extraction.\r\n  \r\n3. **System Integration:**\r\n    - Deploy the system to automatically capture images or video at class entry points or during attendance sessions.\r\n    - Connect the face recognition system to the existing student information database for automated attendance logging.\r\n  \r\n4. **Testing and Validation:**\r\n    - Perform extensive testing using the dataset to ensure accuracy under various environmental conditions.\r\n    - Validate the system with different students, expressions, lighting, and angles to achieve real-time performance and reliability.\r\n\r\n---\r\n\r\n### Tech Stack\r\n\r\n#### Frontend:\r\n\r\n- **React.js** (for user interface development and real-time status visualization).\r\n- **HTML5/CSS3** (for basic layout and design).\r\n\r\n#### Backend:\r\n\r\n- **Node.js** (for server-side logic, handling API requests, and connecting to the database).\r\n- **Express.js** (for setting up RESTful APIs to manage facial recognition requests and responses).\r\n- **Python 3.x** (for image processing, face recognition, and machine learning model implementation).\r\n\r\n#### Machine Learning & Computer Vision:\r\n\r\n- **TensorFlow** (for building and training the neural network models).\r\n- **Keras** (for high-level neural network API, making model development faster).\r\n- **OpenCV** (for real-time face detection and image processing).\r\n- **Dlib** (for face detection and feature extraction).\r\n- **FaceNet or VGGFace2** (for facial recognition using pre-trained deep learning models).\r\n\r\n#### Database:\r\n\r\n- **MongoDB** (for storing student facial data, names, and registration numbers in a NoSQL database).\r\n- **Firebase Firestore** (for real-time database management, enabling quick updates and querying).\r\n\r\n#### Cloud & Deployment:\r\n\r\n- **Google Cloud Platform (GCP)** (for hosting the machine learning model and database).\r\n- **Docker** (for containerizing the application, ensuring it runs consistently across environments).\r\n- **Heroku** or **AWS EC2** (for deploying the application to the cloud).\r\n\r\n#### Version Control & Collaboration:\r\n\r\n- **Git** (for version control).\r\n- **GitHub** (for collaboration, issue tracking, and project management).\r\n\r\n---\r\n\r\n### Key Features\r\n\r\n1. **Real-time Face Detection:**\r\n    - Detect student faces in real-time with high accuracy, using optimized deep learning models.\r\n  \r\n2. **Automated Attendance:**\r\n    - Automatically log student attendance upon identification, reducing manual errors.\r\n  \r\n3. **High Accuracy Identification:**\r\n    - Accurately classify students by matching facial data to their name and registration number stored in the database.\r\n  \r\n4. **Scalability:**\r\n    - The system is designed to be scalable, allowing for future expansion to include additional classes, departments, or even the entire campus.\r\n\r\n---\r\n\r\n### Novelty Points\r\n\r\n- **Automated Attendance:** The system provides an automated and accurate alternative to traditional manual roll calls, improving efficiency.\r\n- **Real-Time Monitoring:** It offers real-time monitoring of student presence in the classroom.\r\n- **Expandable:** The system can easily be expanded beyond the current classroom to other parts of the campus or to a larger number of students.\r\n\r\n---\r\n\r\n### Conclusion\r\n\r\nThis face detection and recognition system offers a secure, efficient, and automated solution for identifying students and managing classroom attendance. With its real-time processing, it significantly reduces administrative work while providing an accurate and reliable method for monitoring student presence. The system‚Äôs scalability also allows for future expansion across different classes or departments, making it a valuable addition to any educational institution.\r\n\r\n---\r\n\r\n### Future Work\r\n\r\n- Integrate with **IoT devices** for additional capabilities such as door access control or security cameras.\r\n- Explore **multimodal biometric systems** that combine face recognition with fingerprint or voice recognition for enhanced security.\r\n- Extend the system to manage not just attendance but also student performance tracking and reporting.\r\n\r\n---\r\n\r\n### Research Papers Reviewed\r\n\r\n- [Face Recognition: A Literature Survey](https://arxiv.org/pdf/0812.02575)\r\n- [A Review of Face Recognition Technology](https://www.researchgate.net/publication/343118558_A_Review_of_Face_Recognition_Technology)\r\n- [A survey on deep learning techniques for image and video semantic segmentation](https://www.sciencedirect.com/science/article/pii/S2590005619300141)\r\n- [Deep Learning for Face Recognition: A Critical Analysis](https://ieeexplore.ieee.org/document/8356995)\r\n- [Face Recognition System: A Comprehensive Study](https://www.ijert.org/research/face-recognition-system-IJERTV8IS050150.pdf)\r\n- [Face Recognition Based Attendance Management System: A Comprehensive Review](https://www.sciencedirect.com/science/article/pii/S1877050923000546)\r\n- [Face Recognition Techniques: A Comprehensive Review](https://www.sciencegate.app/keyword/502229)\r\n- [A Survey on Deep Face Recognition in the Wild: Recent Advances and New Frontiers](https://www.mdpi.com/1424-8220/20/2/342)\r\n- [Face Recognition Methods and Their Applications: A Review](https://link.springer.com/article/10.1007/s10559-024-00655-w)\r\n- [A comprehensive survey on deep learning-based face recognition: From traditional to advanced approaches](https://www.sciencedirect.com/science/article/pii/S2542660524000313)\r\n\r\n### Dataset\r\n- [Dataset](https://www.notion.so/Dataset-10cb23a0120e8048949af7f6b39b5c12?pvs=21)\r\n\r\n### Workflow Diagram\r\n- [WorkFlow Diagram](https://www.notion.so/WorkFlow-Diagram-10cb23a0120e8088954fccd6831b9dbb?pvs=21)\r\n",
    "topics": [],
    "language": "Python"
  },
  {
    "name": "FacultyConnect-AI",
    "readme": "# Faculty-Insights-Hub-Research-Analysis-Analytics-Dashboard-AI-Chatbot\n**Faculty Insights Hub** is a comprehensive system that analyzes faculty research profiles, provides interactive analytics dashboards, and integrates an AI-powered chatbot for faculty-related queries. It leverages web scraping, NLP, MongoDB, and visualization tools to enhance faculty data accessibility and research collaboration insights. \n",
    "topics": [
      "ai",
      "analytics",
      "nlp-machine-learning",
      "research",
      "webscraping"
    ],
    "language": "Python"
  },
  {
    "name": "FinanceTavily",
    "readme": "# Finance Question Answering System with LangGraph\n\n## Overview\n\nThis project implements a finance-focused question answering system using LangGraph, which intelligently routes questions to either a finance-specific search or a fallback response based on the question content.\n\n## Key Components\n\n### LangChain Ecosystem\n- **LangChain**: Provides the foundational framework for building language model applications\n- **LangGraph**: Enables creating stateful, multi-actor applications with cyclic workflows\n- **LangChain Community**: Offers integrations with third-party tools and services\n\nThese tools work together to create a flexible pipeline that can:\n1. Classify incoming questions\n2. Route them to appropriate handlers\n3. Process finance questions with external search\n4. Provide graceful fallbacks for non-finance questions\n\n### Tavily Integration\nTavily is used as the search API because it:\n- Provides high-quality, curated search results\n- Offers specialized knowledge in finance/technical domains\n- Has clean API responses that are easy to parse\n- Includes features like search depth control and result filtering\n\n## Installation\n\n```bash\npip install langchain langgraph langchain-community tavily-python\n```\n  \n## System Flow\n\n```mermaid\ngraph TD\n    A[Start: User Question] --> B[Classifier Node]\n    B -->|Finance Question| C[Finance Tavily Search]\n    B -->|Non-Finance| D[Fallback Response]\n    C --> E[Output Answer]\n    D --> E\n    E --> F[End]\n```\n\n## Usage\n - Set your Tavily API key in the script\n\n - Run the application: python llm.py\n \n - Enter finance-related questions when prompted\n\n",
    "topics": [],
    "language": "Python"
  },
  {
    "name": "Github-repos-3D-graph-representation",
    "readme": "# Github-repos-3D-graph-representation\nGitHub repos as a 3D graph based on README content similarity.\n",
    "topics": [],
    "language": null
  },
  {
    "name": "InterviewPrep",
    "readme": "",
    "topics": [],
    "language": null
  },
  {
    "name": "Its-algober",
    "readme": "# **It's Algober: Your Hacktoberfest Starter Pack**\n\n<a href=\"https://hacktoberfest.com/\" ><img src=\"Its-hacktober Banner.png\" ></a>\n\n> Tap the banner to know more about Hacktoberfest 2024.\n\n<div align=\"center\">\n  <a href=\"https://github.com/sudo-parnab/Its-apptober\" style=\"text-decoration: none; margin-right: 20px;\">\n    <img src=\"https://img.shields.io/badge/App%20Development-%2350D94C?style=for-the-badge&logo=app&logoColor=white\" alt=\"App Development\" style=\"border-radius: 7px;\">\n  </a>\n   <a href=\"https://github.com/sudo-parnab/Its-algober\" style=\"text-decoration: none;\">\n    <img src=\"https://img.shields.io/badge/DSA%20and%20Coding-%23234B21?style=for-the-badge&logo=code&logoColor=white\" alt=\"DSA and Coding\" style=\"border-radius: 7px;\">\n  </a>\n   <a href=\"https://github.com/sudo-parnab/Its-webtober\" style=\"text-decoration: none; margin-right: 20px;\">\n    <img src=\"https://img.shields.io/badge/Web%20Development-%2350D94C?style=for-the-badge&logo=web&logoColor=white\" alt=\"Web Development\" style=\"border-radius: 7px;\">\n  </a>\n</div>\n\n### Hey there, future programmer! üë®‚Äçüíª\n\nWelcome to our community! Whether you're a seasoned programmer, a dedicated DSA enthusiast, or just starting your journey, we're glad you're here.\n\n**Don't worry if you're new to open source.** Everyone starts somewhere! We're here to support and guide you every step of the way. \n\n>  <a href=\"https://chat.whatsapp.com/HfqDn52yy6l8T2d6fL0hKU\" ></a><img src=\"https://logos-world.net/wp-content/uploads/2020/05/Logo-WhatsApp.png\" width=\"22px\" >**Join our [Its Hacktober WhatsApp Group](https://chat.whatsapp.com/HfqDn52yy6l8T2d6fL0hKU) for more updates.**</a>\n\n### **How to Get Started:**\n\n1. **Star this repository, By starring, you can keep track of its progress and updates.**\n   \n2. **Add Your Algorithm/Code:** Contribute a new code addressing all programmers need. Follow the detailed instructions outlined in our [CONTRIBUTING.md](CONTRIBUTING.md) to ensure your project aligns with our guidelines.\n\n3. **Improve Existing Algorithm/Codes:**  Browse the repository and find an existing code you'd like to contribute to. Check the project's specific instructions or open issues to understand current needs. Remember to follow the general contribution guidelines documented in [CONTRIBUTING.md](CONTRIBUTING.md).\n\n4. **Once you've made your changes, submit a pull request and let us review your work.**\n\n## **Contributions are welcome in all areas of data structures and algorithms, including:**\n\n* **Fundamental data structures:** Implement and analyze fundamental data structures like arrays, linked lists, stacks, queues, trees, graphs, and hash tables.\n* **Sorting and searching algorithms:** Develop efficient algorithms for sorting and searching data, such as quicksort, mergesort, bubble sort, binary search, and linear search.\n* **Dynamic programming:** Solve complex problems by breaking them down into smaller subproblems and using memoization or tabulation to avoid redundant calculations.\n* **Greedy algorithms:** Design algorithms that make locally optimal choices at each step, hoping to achieve a globally optimal solution.\n* **Divide-and-conquer algorithms:** Divide problems into smaller, similar subproblems, solve the subproblems recursively, and combine the solutions to solve the original problem.\n* **Graph algorithms:** Explore algorithms for traversing graphs, finding shortest paths, detecting cycles, and solving problems related to networks and relationships.\n* **Advanced data structures:** Implement and analyze more advanced data structures like tries, heaps, segment trees, and Fenwick trees.\n* **Competitive programming:** Practice solving algorithmic problems under time constraints and improve your problem-solving skills.\n\n**We encourage contributions from developers at all levels of experience, regardless of background or skillset.** Your unique perspective and contributions can make a valuable impact on our projects.\n\n### **Encountering Issues?**\n\nIf you're facing any problems, ask on [Its Hacktober WhatsApp Group](https://chat.whatsapp.com/HfqDn52yy6l8T2d6fL0hKU)\n\nOur community is here to help! Feel free to ask questions, seek advice, or collaborate on solutions.\n\n**Let's have some fun and learn together!** üéâ\n",
    "topics": [],
    "language": null
  },
  {
    "name": "Its-apptober",
    "readme": "# **It's Apptober: Your Hacktoberfest Starter Pack**\n\n<a href=\"https://hacktoberfest.com/\" ><img src=\"Its-hacktober Banner.png\" ></a>\n\n> Tap the banner to know more about Hacktoberfest 2024.\n\n<div align=\"center\">\n   <a href=\"https://github.com/sudo-parnab/Its-webtober\" style=\"text-decoration: none; margin-right: 20px;\">\n    <img src=\"https://img.shields.io/badge/Web%20Development-%2350D94C?style=for-the-badge&logo=web&logoColor=white\" alt=\"Web Development\" style=\"border-radius: 7px;\">\n  </a>\n  <a href=\"https://github.com/sudo-parnab/Its-apptober\" style=\"text-decoration: none; margin-right: 20px;\">\n    <img src=\"https://img.shields.io/badge/App%20Development-%23234B21?style=for-the-badge&logo=app&logoColor=white\" alt=\"App Development\" style=\"border-radius: 7px;\">\n  </a>\n  <a href=\"https://github.com/sudo-parnab/Its-algober\" style=\"text-decoration: none;\">\n    <img src=\"https://img.shields.io/badge/DSA%20and%20Coding-%2350D94C?style=for-the-badge&logo=code&logoColor=white\" alt=\"DSA and Coding\" style=\"border-radius: 7px;\">\n  </a>\n</div>\n\n### Hey there, future web wizard! üßô‚Äç‚ôÇÔ∏è\n\nThanks for stopping by! I am so glad you're here. Whether you're a seasoned developer or just starting out, there's a place for you in our community.\n\n**Don't worry if you're new to open source.** Everyone starts somewhere! We're here to support and guide you every step of the way. \n\n >  <a href=\"https://chat.whatsapp.com/HfqDn52yy6l8T2d6fL0hKU\" ></a><img src=\"https://logos-world.net/wp-content/uploads/2020/05/Logo-WhatsApp.png\" width=\"22px\" >**Join our [Its Hacktober WhatsApp Group](https://chat.whatsapp.com/HfqDn52yy6l8T2d6fL0hKU) for more updates.**</a> \n\n### **How to Get Started:**\n\n1. **Star this repository, By starring, you can keep track of its progress and updates.**\n   \n2. **Add Your Own Project:** Create a new project addressing a app development need. Follow the detailed instructions outlined in our [CONTRIBUTING.md](CONTRIBUTING.md) to ensure your project aligns with our guidelines.\n\n3. **Improve Existing Projects:**  Browse the repository and find an existing project you'd like to contribute to. Check the project's specific instructions or open issues to understand current needs. Remember to follow the general contribution guidelines documented in [CONTRIBUTING.md](CONTRIBUTING.md).\n\n4. **Once you've made your changes, submit a pull request and let us review your work.**\n\n## **Contributions are welcome in all areas of application development, including:**\n\n* **Mobile app development:** Create native apps for iOS and Android using frameworks like Swift, Objective-C, Kotlin, or Flutter.\n* **Desktop app development:** Build applications for Windows, macOS, and Linux using technologies like Electron, Qt, or native frameworks.\n* **Game development:** Develop games using game engines like Unity, Unreal Engine, or Godot.\n* **Cross-platform development:** Create apps that run on multiple platforms using frameworks like React Native, Flutter, or Xamarin.\n* **Cloud-native development:** Build scalable and distributed applications using cloud platforms like AWS, Azure, or GCP.\n* **DevOps and automation:** Implement DevOps practices and automate tasks using tools like Docker, Kubernetes, and Ansible.\n* **Artificial intelligence and machine learning:** Develop applications that leverage AI and ML techniques to solve complex problems.\n\n**We encourage contributions from developers at all levels of experience, regardless of background or skillset.** Your unique perspective and contributions can make a valuable impact on our projects.\n\n### **Encountering Issues?**\n\nIf you're facing any problems, ask on [Its Hacktober WhatsApp Group](https://chat.whatsapp.com/HfqDn52yy6l8T2d6fL0hKU)\n\nOur community is here to help! Feel free to ask questions, seek advice, or collaborate on solutions.\n\n**Let's have some fun and learn together!** üéâ\n",
    "topics": [],
    "language": null
  },
  {
    "name": "Its-webtober",
    "readme": "# **It's Webtober: Your Hacktoberfest Starter Pack**\n\n<a href=\"https://hacktoberfest.com/\">\n  <img src=\"./Banner.png\" alt=\"Hacktoberfest Banner\">\n</a>\n\n> Tap the banner to know more about Hacktoberfest 2024.\n\n<div align=\"center\">\n  <a href=\"https://github.com/sudo-parnab/Its-apptober\" style=\"text-decoration: none; margin-right: 20px;\">\n    <img src=\"https://img.shields.io/badge/App%20Development-%2350D94C?style=for-the-badge&logo=app&logoColor=white\" alt=\"App Development\" style=\"border-radius: 7px;\">\n  </a>\n   <a href=\"https://github.com/sudo-parnab/Its-webtober\" style=\"text-decoration: none; margin-right: 20px;\">\n    <img src=\"https://img.shields.io/badge/Web%20Development-%23234B21?style=for-the-badge&logo=web&logoColor=white\" alt=\"Web Development\" style=\"border-radius: 7px;\">\n  </a>\n  <a href=\"https://github.com/sudo-parnab/Its-algober\" style=\"text-decoration: none;\">\n    <img src=\"https://img.shields.io/badge/DSA%20and%20Coding-%2350D94C?style=for-the-badge&logo=code&logoColor=white\" alt=\"DSA and Coding\" style=\"border-radius: 7px;\">\n  </a>\n</div>\n\n### Hey there, future web wizard! üßô‚Äç‚ôÇÔ∏è\n\nThanks for stopping by! I am so glad you're here. Whether you're a seasoned developer or just starting out, there's a place for you in our community.\n\n**Don't worry if you're new to open source.** Everyone starts somewhere! We're here to support and guide you every step of the way. \n\n>  <a href=\"https://chat.whatsapp.com/HfqDn52yy6l8T2d6fL0hKU\" ></a><img src=\"https://logos-world.net/wp-content/uploads/2020/05/Logo-WhatsApp.png\" width=\"22px\" >**Join our [Its Hacktober WhatsApp Group](https://chat.whatsapp.com/HfqDn52yy6l8T2d6fL0hKU) for more updates.**</a>\n\n### **How to Get Started:**\n\n1. **Star this repository, By starring, you can keep track of its progress and updates.**\n   \n2. **Add Your Own Project:** Create a new project addressing a web development need. Follow the detailed instructions outlined in our [CONTRIBUTING.md](CONTRIBUTING.md) to ensure your project aligns with our guidelines.\n\n3. **Improve Existing Projects:**  Browse the repository and find an existing project you'd like to contribute to. Check the project's specific instructions or open issues to understand current needs. Remember to follow the general contribution guidelines documented in [CONTRIBUTING.md](CONTRIBUTING.md).\n\n4. **Once you've made your changes, submit a pull request and let us review your work.**\n\n## **Contributions are welcome in all areas of web development, including:**\n\n* **Front-end development:** Create visually appealing and user-friendly interfaces using HTML, CSS, and JavaScript. Explore popular frameworks like React, Angular, or Vue to streamline development and build complex applications.\n* **Back-end development:** Develop the server-side logic and functionality of web applications using languages and frameworks like Node.js, Python (Django, Flask), or Ruby on Rails. Handle data storage, processing, and API interactions.\n* **Full-stack development:** Demonstrate your expertise in both front-end and back-end development by building complete web applications from start to finish. Combine your skills to create seamless user experiences and efficient server-side operations.\n* **Mobile web development:** Create responsive websites that are optimized for mobile devices and provide a great user experience on smaller screens.\n* **Accessibility development:** Ensure your websites are accessible to users with disabilities by following accessibility guidelines like WCAG.\n* **Performance optimization:** Improve website load times and performance using techniques like code minification, image optimization, and caching.\n* **Security development:** Protect your websites from vulnerabilities and attacks by implementing security best practices and using security tools.\n\n**We encourage contributions from developers at all levels of experience, regardless of background or skillset.** Your unique perspective and contributions can make a valuable impact on our projects.\n\n### **Encountering Issues?**\n\nIf you're facing any problems, ask on [Its Hacktober WhatsApp Group](https://chat.whatsapp.com/HfqDn52yy6l8T2d6fL0hKU)\n\nOur community is here to help! Feel free to ask questions, seek advice, or collaborate on solutions.\n\n**Let's have some fun and learn together!** üéâ\n",
    "topics": [],
    "language": "HTML"
  },
  {
    "name": "Mental-health-chatbot--Forest-Haven",
    "readme": "# Forest Therapy: A Mental Health Chatbot\n\nForest Therapy is an intelligent mental wellness chatbot built using PyTorch, Flask, and MongoDB. Designed to provide emotional support and mood tracking, this forest-themed chatbot ‚Äî affectionately named **Zara** ‚Äî engages in conversations, tracks user mood, and performs basic sentiment analysis.\n\n---\n\n## üß† Features\n\n- ü§ñ AI-powered chatbot using a trained neural network  \n- üó®Ô∏è Natural language understanding via NLTK and PyTorch  \n- üìä Mood tracking and sentiment analysis  \n- üí¨ Personalized conversation history  \n- üìÅ MongoDB integration for persistent storage  \n- üåê Web interface powered by Flask  \n- üß© Session-based tracking for individual users  \n---\n\n## Dataset\n\n - Refer the intents.json file.\n\n---\n\n## üîß Tech Stack\n\n### üíª Backend\n- **Python 3**\n- **Flask** ‚Äì Web framework for serving chatbot and handling API routes\n- **Flask-Session** ‚Äì User session management\n\n### ü§ñ Machine Learning\n- **PyTorch** ‚Äì Neural network model training and inference\n- **NLTK** ‚Äì Tokenization and stemming\n- **TextBlob** ‚Äì Sentiment analysis\n\n### üóÉÔ∏è Database\n- **MongoDB** ‚Äì Stores conversation history, mood entries, and sentiment analysis\n\n### üåê Frontend\n- **HTML (Jinja2 template)** ‚Äì Simple UI with `base.html`\n\n### üì¶ Other Tools\n- **TorchScript** ‚Äì Saves trained model (`data.pth`)\n- **JSON** ‚Äì Used for defining intents and patterns (`intents.json`)\n---\n\n## üöÄ Workflow\nThe complete lifecycle of the Forest Therapy chatbot consists of the following stages:\n\n### üèóÔ∏è 1. Data Preparation\n- Define conversation intents in `intents.json`\n- Each intent contains:\n  - `tag`: intent label\n  - `patterns`: sample user messages\n  - `responses`: possible bot replies\n\n### üß† 2. Model Training (`train.py`)\n- Tokenize and stem all patterns using NLTK\n- Convert text to bag-of-words vectors\n- Train a neural network using PyTorch\n- Save model as `data.pth` for later inference\n\n### üß™ 3. Chatbot Inference (`chat.py`)\n- Load the trained model and `intents.json`\n- For each user message:\n  - Tokenize and convert to bag-of-words\n  - Predict intent and choose a random response\n  - If confidence < 0.75, return fallback message\n\n### üåê 4. Web Integration (`app.py`)\n- Set up Flask server with session-based user ID\n- Routes:\n  - `/predict`: Accepts user message and returns bot response\n  - `/mood`: Accepts user mood (1‚Äì10) and stores it\n  - `/analysis`: Returns mood and sentiment trends\n- Stores all data in MongoDB (`conversations`, `mood_scores`, `analysis`)\n\n### üìä 5. Mood & Sentiment Analysis\n- Every 5th query, calculate:\n  - Average mood from user submissions\n  - Sentiment score using TextBlob polarity\n- Store the computed metrics in MongoDB\n\n### üßæ 6. Frontend (HTML Template)\n- Renders basic UI for chatting with the bot\n- Displays real-time responses and mood input option\n\n![Forest Haven Chatbot Preview](https://github.com/us107/Mental-health-chatbot--Forest-Haven/blob/main/image.png?raw=true)\n\n---\n\n## üé• Demo\n\nWatch the chatbot in action!  \n[‚ñ∂Ô∏è Click to view demo video](https://github.com/us107/Mental-health-chatbot--Forest-Haven/blob/630992ea1c01be5f24b5f258127ba8b1de42d994/demo.mp4)\n---\n\n## üõ†Ô∏è How to Run This Locally\n\n1. Clone the repo:\n   ```bash\n   git clone https://github.com/your-username/forest-haven.git\n   cd forest-haven\n2. Create a virtual env\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # On Windows use: venv\\Scripts\\activate\n\n3. Install dependencies\n    ```bash\n     pip install -r requirements.txt\n\n4. Run train.py and then chat.py\n    ```bash\n    python train.py\n    python chat.py \n\n6. Run the Flask app\n   ```bash\n      python app.py\n\n## Contributing \n   Contributions are welcome! Feel free to open issues or pull requests. Let's grow Forest Haven together and support more minds! üíö\n\n   \n\n",
    "topics": [],
    "language": "Python"
  },
  {
    "name": "Micro-Gas-Turbine-electrical-energy-Prediction",
    "readme": "# Micro-Gas-Turbine-electrical-energy-Prediction",
    "topics": [],
    "language": "Jupyter Notebook"
  },
  {
    "name": "MyPortfolio",
    "readme": "# MyPortfolio\nPersonal portfolio website showcasing my projects, skills, and experience. Built with HTML, CSS and JavaScript. Includes interactive sections for projects, about me, contact form, and social links. Deployed on Vercel.\n",
    "topics": [],
    "language": "HTML"
  },
  {
    "name": "Olympics_medals",
    "readme": "# Project title\r\nPredicting Country Medal Counts: Unveiling Patterns in Historical and Current Sports Data\r\n\r\n# Description\r\nThis project aims to predict a country's future medal count in sports events using machine learning models. By analyzing a combination of historical and current data, including factors such as past medal counts, athlete participation, demographics, and other relevant features, the goal is to identify patterns contributing to a country's success. The hypothesis is that a well-trained machine learning model, leveraging a comprehensive dataset, can provide accurate predictions for a country's medal count in upcoming sports events. This analysis enhances our understanding of the diverse factors influencing a country's performance in sports competitions.\r\n\r\n# Machine learning project steps\r\nMost machine learning projects typically followed a similar outline, and I also adopted this structure. This framework proved effective in addressing various machine learning challenges.\r\n\r\n**Project Steps**\r\n\r\n1. Form a hypothesis.\r\n2. Find and explore the data.\r\n3. (If necessary) Reshape the data to predict your target.\r\n4. Clean the data for ML.\r\n5. Pick an error metric.\r\n6. Split your data.\r\n7. Train a model.\r\n\r\n# Code\r\n\r\n-[Main code here](teams.ipynb)\r\n,[Dataset here](teams.csv)\r\n\r\n### Machine Learning Models Used\r\n\r\n#### 1. Linear Regression\r\n\r\nI employed Linear Regression to analyze and predict the relationship between variables in the dataset. This model is well-suited for scenarios where a linear relationship exists between the input features and the target variable.\r\n\r\n#### 2. K-Nearest Neighbors (KNN)\r\n\r\nAdditionally, the K-Nearest Neighbors algorithm was utilized to make predictions based on the similarity of data points. KNN is a versatile algorithm that doesn't assume a specific structure in the data, making it suitable for various types of datasets.\r\n\r\n## Visualization of models\r\n#### ![error_ratio](https://github.com/us107/Olympics_medals/blob/main/error.png)\r\n\r\n### ![KNN_reg](https://github.com/us107/Olympics_medals/blob/main/kn.png)\r\n\r\n\r\n\r\n## üîó Links\r\n[![linkedin](https://img.shields.io/badge/linkedin-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/trisha-sharma-871544251)\r\n",
    "topics": [
      "jupyter-notebook"
    ],
    "language": "Jupyter Notebook"
  },
  {
    "name": "plagiarism-checker",
    "readme": "# plag\n \n",
    "topics": [
      "html",
      "python"
    ],
    "language": null
  },
  {
    "name": "Pomodoro-timer",
    "readme": "# Pomodoro Timer\n\nThe Pomodoro Timer is a web-based application designed to enhance productivity using the Pomodoro Technique. It breaks work into 25-minute intervals (\"Pomodoros\") with short breaks in between. After four Pomodoros, a longer break is taken.\n\n# Techstack\n- html\n- CSS \n- JavaScript\n\n# Usage\n1. Clone this repository.\n2. Open index.html in your browser:\n\n- Right clicking on index.html file in your project explorer.\n- If you have Live server,you can open with that by also right clicking the index.html file.\n\n3. Features:\n\n- Timer Display: Shows minutes and seconds remaining in the current session.\n- Buttons: Click buttons (Work, Short Break, Long Break, Stop) to start different timer sessions or stop the timer.\n- Animated Icon: An animated icon visually indicates the timer's activity.\n\n# Main Learnings\n- Implemented timer logic using JavaScript setTimeout for countdown functionality.\n- Used CSS for styling, including grid layout for button arrangement and animations.\n- Explored basic DOM manipulation for updating timer display dynamically.\n- Gained familiarity with GitHub for version control and project hosting.\n\n# Website Link\n-[ http://127.0.0.1:5500/index.html](https://heroic-kangaroo-fc46c6.netlify.app/)\n# Author\nTrisha Sharma\n\n# License\nThis project is licensed under the MIT License.\n\n",
    "topics": [
      "css",
      "html",
      "javascript"
    ],
    "language": "CSS"
  },
  {
    "name": "reflex",
    "readme": "```diff\n+ Searching for Pynecone? You are in the right repo. Pynecone has been renamed to Reflex. +\n```\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex_dark.svg#gh-light-mode-only\" alt=\"Reflex Logo\" width=\"300px\">\n<img src=\"https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex_light.svg#gh-dark-mode-only\" alt=\"Reflex Logo\" width=\"300px\">\n\n<hr>\n\n### **‚ú® Performant, customizable web apps in pure Python. Deploy in seconds. ‚ú®**\n[![PyPI version](https://badge.fury.io/py/reflex.svg)](https://badge.fury.io/py/reflex)\n![versions](https://img.shields.io/pypi/pyversions/reflex.svg)\n[![Documentation](https://img.shields.io/badge/Documentation%20-Introduction%20-%20%23007ec6)](https://reflex.dev/docs/getting-started/introduction)\n[![Discord](https://img.shields.io/discord/1029853095527727165?color=%237289da&label=Discord)](https://discord.gg/T5WSbC2YtQ)\n</div>\n\n---\n\n[English](https://github.com/reflex-dev/reflex/blob/main/README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/reflex-dev/reflex/blob/main/docs/zh/zh_cn/README.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/reflex-dev/reflex/blob/main/docs/zh/zh_tw/README.md) | [T√ºrk√ße](https://github.com/reflex-dev/reflex/blob/main/docs/tr/README.md) | [‡§π‡§ø‡§Ç‡§¶‡•Ä](https://github.com/reflex-dev/reflex/blob/main/docs/in/README.md) | [Portugu√™s (Brasil)](https://github.com/reflex-dev/reflex/blob/main/docs/pt/pt_br/README.md) | [Italiano](https://github.com/reflex-dev/reflex/blob/main/docs/it/README.md) | [Espa√±ol](https://github.com/reflex-dev/reflex/blob/main/docs/es/README.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/reflex-dev/reflex/blob/main/docs/kr/README.md) | [Êó•Êú¨Ë™û](https://github.com/reflex-dev/reflex/blob/main/docs/ja/README.md) | [Deutsch](https://github.com/reflex-dev/reflex/blob/main/docs/de/README.md) | [Persian (Ÿæÿßÿ±ÿ≥€å)](https://github.com/reflex-dev/reflex/blob/main/docs/pe/README.md) | [Ti·∫øng Vi·ªát](https://github.com/reflex-dev/reflex/blob/main/docs/vi/README.md)\n\n---\n\n# Reflex\n\nReflex is a library to build full-stack web apps in pure Python.\n\nKey features:\n* **Pure Python** - Write your app's frontend and backend all in Python, no need to learn Javascript.\n* **Full Flexibility** - Reflex is easy to get started with, but can also scale to complex apps.\n* **Deploy Instantly** - After building, deploy your app with a [single command](https://reflex.dev/docs/hosting/deploy-quick-start/) or host it on your own server.\n\nSee our [architecture page](https://reflex.dev/blog/2024-03-21-reflex-architecture/#the-reflex-architecture) to learn how Reflex works under the hood.\n\n## ‚öôÔ∏è Installation\n\nOpen a terminal and run (Requires Python 3.9+):\n\n```bash\npip install reflex\n```\n\n## ü•≥ Create your first app\n\nInstalling `reflex` also installs the `reflex` command line tool.\n\nTest that the install was successful by creating a new project. (Replace `my_app_name` with your project name):\n\n```bash\nmkdir my_app_name\ncd my_app_name\nreflex init\n```\n\nThis command initializes a template app in your new directory. \n\nYou can run this app in development mode:\n\n```bash\nreflex run\n```\n\nYou should see your app running at http://localhost:3000.\n\nNow you can modify the source code in `my_app_name/my_app_name.py`. Reflex has fast refreshes so you can see your changes instantly when you save your code.\n\n\n## ü´ß Example App\n\nLet's go over an example: creating an image generation UI around [DALL¬∑E](https://platform.openai.com/docs/guides/images/image-generation?context=node). For simplicity, we just call the [OpenAI API](https://platform.openai.com/docs/api-reference/authentication), but you could replace this with an ML model run locally.\n\n&nbsp;\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle.gif\" alt=\"A frontend wrapper for DALL¬∑E, shown in the process of generating an image.\" width=\"550\" />\n</div>\n\n&nbsp;\n\nHere is the complete code to create this. This is all done in one Python file!\n\n\n  \n```python\nimport reflex as rx\nimport openai\n\nopenai_client = openai.OpenAI()\n\n\nclass State(rx.State):\n    \"\"\"The app state.\"\"\"\n\n    prompt = \"\"\n    image_url = \"\"\n    processing = False\n    complete = False\n\n    def get_image(self):\n        \"\"\"Get the image from the prompt.\"\"\"\n        if self.prompt == \"\":\n            return rx.window_alert(\"Prompt Empty\")\n\n        self.processing, self.complete = True, False\n        yield\n        response = openai_client.images.generate(\n            prompt=self.prompt, n=1, size=\"1024x1024\"\n        )\n        self.image_url = response.data[0].url\n        self.processing, self.complete = False, True\n\n\ndef index():\n    return rx.center(\n        rx.vstack(\n            rx.heading(\"DALL-E\", font_size=\"1.5em\"),\n            rx.input(\n                placeholder=\"Enter a prompt..\",\n                on_blur=State.set_prompt,\n                width=\"25em\",\n            ),\n            rx.button(\n                \"Generate Image\", \n                on_click=State.get_image,\n                width=\"25em\",\n                loading=State.processing\n            ),\n            rx.cond(\n                State.complete,\n                rx.image(src=State.image_url, width=\"20em\"),\n            ),\n            align=\"center\",\n        ),\n        width=\"100%\",\n        height=\"100vh\",\n    )\n\n# Add state and page to the app.\napp = rx.App()\napp.add_page(index, title=\"Reflex:DALL-E\")\n```\n\n\n\n\n\n## Let's break this down.\n\n<div align=\"center\">\n<img src=\"docs/images/dalle_colored_code_example.png\" alt=\"Explaining the differences between backend and frontend parts of the DALL-E app.\" width=\"900\" />\n</div>\n\n\n### **Reflex UI**\n\nLet's start with the UI.\n\n```python\ndef index():\n    return rx.center(\n        ...\n    )\n```\n\nThis `index` function defines the frontend of the app.\n\nWe use different components such as `center`, `vstack`, `input`, and `button` to build the frontend. Components can be nested within each other\nto create complex layouts. And you can use keyword args to style them with the full power of CSS.\n\nReflex comes with [60+ built-in components](https://reflex.dev/docs/library) to help you get started. We are actively adding more components, and it's easy to [create your own components](https://reflex.dev/docs/wrapping-react/overview/).\n\n### **State**\n\nReflex represents your UI as a function of your state.\n\n```python\nclass State(rx.State):\n    \"\"\"The app state.\"\"\"\n    prompt = \"\"\n    image_url = \"\"\n    processing = False\n    complete = False\n\n```\n\nThe state defines all the variables (called vars) in an app that can change and the functions that change them.\n\nHere the state is comprised of a `prompt` and `image_url`. There are also the booleans `processing` and `complete` to indicate when to disable the button (during image generation) and when to show the resulting image.\n\n### **Event Handlers**\n\n```python\ndef get_image(self):\n    \"\"\"Get the image from the prompt.\"\"\"\n    if self.prompt == \"\":\n        return rx.window_alert(\"Prompt Empty\")\n\n    self.processing, self.complete = True, False\n    yield\n    response = openai_client.images.generate(\n        prompt=self.prompt, n=1, size=\"1024x1024\"\n    )\n    self.image_url = response.data[0].url\n    self.processing, self.complete = False, True\n```\n\nWithin the state, we define functions called event handlers that change the state vars. Event handlers are the way that we can modify the state in Reflex. They can be called in response to user actions, such as clicking a button or typing in a text box. These actions are called events.\n\nOur DALL¬∑E. app has an event handler, `get_image` to which get this image from the OpenAI API. Using `yield` in the middle of an event handler will cause the UI to update. Otherwise the UI will update at the end of the event handler.\n\n### **Routing**\n\nFinally, we define our app.\n\n```python\napp = rx.App()\n```\n\nWe add a page from the root of the app to the index component. We also add a title that will show up in the page preview/browser tab.\n\n```python\napp.add_page(index, title=\"DALL-E\")\n```\n\nYou can create a multi-page app by adding more pages.\n\n## üìë Resources\n\n<div align=\"center\">\n\nüìë [Docs](https://reflex.dev/docs/getting-started/introduction) &nbsp; |  &nbsp; üóûÔ∏è [Blog](https://reflex.dev/blog) &nbsp; |  &nbsp; üì± [Component Library](https://reflex.dev/docs/library) &nbsp; |  &nbsp; üñºÔ∏è [Gallery](https://reflex.dev/docs/gallery) &nbsp; |  &nbsp; üõ∏ [Deployment](https://reflex.dev/docs/hosting/deploy-quick-start)  &nbsp;   \n\n</div>\n\n\n## ‚úÖ Status\n\nReflex launched in December 2022 with the name Pynecone.\n\nAs of February 2024, our hosting service is in alpha! During this time anyone can deploy their apps for free. See our [roadmap](https://github.com/reflex-dev/reflex/issues/2727) to see what's planned.\n\nReflex has new releases and features coming every week! Make sure to :star: star and :eyes: watch this repository to stay up to date.\n\n## Contributing\n\nWe welcome contributions of any size! Below are some good ways to get started in the Reflex community.\n\n-   **Join Our Discord**: Our [Discord](https://discord.gg/T5WSbC2YtQ) is the best place to get help on your Reflex project and to discuss how you can contribute.\n-   **GitHub Discussions**: A great way to talk about features you want added or things that are confusing/need clarification.\n-   **GitHub Issues**: [Issues](https://github.com/reflex-dev/reflex/issues) are an excellent way to report bugs. Additionally, you can try and solve an existing issue and submit a PR.\n\nWe are actively looking for contributors, no matter your skill level or experience. To contribute check out [CONTIBUTING.md](https://github.com/reflex-dev/reflex/blob/main/CONTRIBUTING.md)\n\n\n## All Thanks To Our Contributors:\n<a href=\"https://github.com/reflex-dev/reflex/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=reflex-dev/reflex\" />\n</a>\n\n## License\n\nReflex is open-source and licensed under the [Apache License 2.0](LICENSE).\n",
    "topics": [],
    "language": null
  },
  {
    "name": "RSA-cryptography",
    "readme": "# RSA Key Generation with Python & Web Crypto API\n\nThis project demonstrates RSA key pair generation using two different methods:\n\n1. **Python Implementation**: Utilizes the `pycryptodome` package to generate RSA key pairs in Python.  \n2. **Web Implementation**: Uses the Web Crypto API to create a simple webpage that generates RSA key pairs directly in the browser.\n\n## Features\n\n- **Python Script**: Generates RSA keys and outputs them in PEM format.  \n- **Webpage**: Provides an interface to generate RSA keys using the Web Crypto API and displays the results.\n\n## Technologies Used\n\n- **Python**: [PyCryptodome](https://pypi.org/project/pycryptodome/)  \n- **Frontend**: HTML, JavaScript, [Web Crypto API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Crypto_API)\n\n## How to Run\n\n1. **Python Script**:\n   - Install the required package:\n     ```bash\n     pip install pycryptodome\n     ```\n   - Run the script:\n     ```bash\n     python rsa_keygen.py\n     ```\n\n2. **Webpage**:\n   - Open `index.html` in your browser to generate RSA keys using the Web Crypto API.\n\n## License\n\nThis project is open-source and available under the [MIT License](LICENSE).\n",
    "topics": [],
    "language": "HTML"
  },
  {
    "name": "Scooter-ordering-System",
    "readme": "",
    "topics": [],
    "language": "CSS"
  },
  {
    "name": "SPHAR-Dataset",
    "readme": "![SPHAR Surveillance Perspective Human Action Recognition Banner](docs/SPHAR-banner.png)\n\n\n# **SPHAR**: **S**urveillance **P**erspective **H**uman **A**ction **R**ecognition Dataset\n\n\n**SPHAR** is a video dataset for **human action recognition**. Its main purpose is to support research in the application area of analyzing activities on public places.\n\nIn this domain, most cameras will share a similar mounting angle and perspective, which we will call the **surveillance perspective** from now on. In **SPHAR**, all videos are shot from this or a similar perspective.\n\nThe videos have been aggregated from multiple sources, converted to a consistent file type (`H265 HEVC .mp4`), cutted and cropped (spatio-temporally) to contain only one action at a time and last but not least sorted into 14 action classes.\n\nThis Repository contains all videos of the SPHAR dataset as well as the [scripts](scripts) needed to create the dataset.\n\n-------\n\nHead over to the [S-SPHAR repository](https://github.com/AlexanderMelde/S-SPHAR-Dataset), if you are looking for a synthetically generated dataset of this perspective.\n\n-------\n\n## Dataset Overview\n\n| # Videos |# Classes | Videos per Class | Video Sources | Dataset Size | Year |\n|----------|----------|------------------|---------------|--------------|------|\n|     7759 |       14 |         8 - 2800 |            11 |       6.2 GB | 2020 |\n\nVideos per class and source dataset:\n\n|source|hitting|kicking|falling|vandalizing|panicking|sitting|walking|running|neutral|luggage|stealing|murdering|carcrash|igniting|license|\n|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|\n|[CAVIAR](http://groups.inf.ed.ac.uk/vision/CAVIAR/CAVIARDATA1/)|4|0|3|0|0|1|61|0|5|5|0|0|0|0|public|\n|[CASIA](http://www.cbsr.ia.ac.cn/english/Action%20Databases%20EN.asp)|12|0|36|14|48|0|204|96|0|0|11|0|0|0|[author's permission](http://www.cbsr.ia.ac.cn/english/Action%20Databases%20EN.asp)|\n|[UCF-Aerial](https://www.crcv.ucf.edu/research/data-sets/ucf-aerial-action/)|0|0|0|0|0|0|71|8|64|0|0|0|0|0|research-only|\n|[UCF-Crime](https://www.crcv.ucf.edu/projects/real-world/)|100|50|0|50|0|0|0|0|0|0|400|50|150|100|research-only|\n|[UT-Interaction](https://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html)|40|20|0|0|0|0|0|0|60|0|0|0|0|0|[MIT](https://opensource.org/licenses/MIT)|\n|[BIT-Interaction](https://sites.google.com/site/alexkongy/software)|100|50|0|0|0|0|0|0|250|0|0|0|0|0|non-commercial|\n|[Live Videos](https://cvrleyva.wordpress.com/2017/04/08/lv-dataset/)|1|0|1|1|4|0|0|0|1|0|7|2|7|1|CC-BY-NC|\n|[UCF-ARG](https://www.crcv.ucf.edu/research/data-sets/ucf-arg/)|144|0|0|144|0|0|288|288|432|0|0|0|0|0|research-only|\n|[VIRAT Ground](https://viratdata.org/)|0|0|0|0|0|208|1111|22|214|0|0|0|0|0|[research & commercial](https://viratdata.org/#getting-data)|\n|[MEVA](https://mevadata.org/)|0|0|0|0|0|22|0|0|0|3|1|0|0|0|[CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/)|\n|[Okutama](http://okutama-action.org/)|0|0|83|0|0|390|1064|102|1170|0|0|0|0|0|[CC-BY-NC-3.0](https://creativecommons.org/licenses/by-nc/3.0/)|\n|[**SPHAR**](https://github.com/AlexanderMelde/SPHAR-Dataset)|**401**|**120**|**123**|**209**|**52**|**621**|**2800**|**516**|**2166**|**8**|**418**|**52**|**157**|**101**|**multiple**|\n\n\n## Example Videos\n| falling | hitting | kicking | luggage | neutral | murdering | sitting | sitting | running |\n|---------|---------|---------|---------|---------|-----------|---------|---------|---------|\n| ![example video of falling class](docs/falling.gif) | ![example video of hitting class](docs/hitting.gif) | ![example video of kicking class](docs/kicking.gif) | ![example video of luggage class](docs/luggage.gif) | ![example video of neutral class](docs/neutral.gif) | ![example video of murdering class](docs/murdering.gif) | ![example video of sitting class](docs/sitting.gif) | ![second example video of sitting class](docs/sitting2.gif) |![example video of running class](docs/running.gif) | \n\n## Known Caveats\nDue to different and missing annotations in the original datasets, not all videos could be automatically cropped to the relevant area containing the action.\n\n- The videos of the UCF-ARG and CASIA datasets are only cutted temporally. \n- The videos of the UCF-Crime, CAVIAR and Live Videos datasets are neither cropped nor cutted and might contain camera changes or watermarks. \n- Videos extracted from the MEVA, Okutama, Bit-Interaction, UCF-Aerial, VIRAT Ground and UT-Interaction datasets are both cropped and cutted to relevant actions, but the quality of the cut heavily depends on the quality of the original annotations and therefore varies by each dataset.\n\nYou can delete the videos you don't need using a simple filename search, as the video names contain abbreviations for each dataset.\n\n## Download\nThe easiest way to just get the dataset videos is by downloading one of our [releases](https://github.com/AlexanderMelde/SPHAR-Dataset/releases):\n\n- [**SPHAR-Dataset-1.0.zip**](https://github.com/AlexanderMelde/SPHAR-Dataset/archive/1.0.zip)\n\nIf you want to modify the dataset and use the conversion and cutting scripts, clone or fork this repository using:\n\n```\ngit clone git@github.com:AlexanderMelde/SPHAR-Dataset.git\n```\n\n## License\nUsing the datasets for researching purposes is possible for all of the videos, but licensing is difficult when aggregating data from multiple sources and licenses.\n\nAll videos of this dataset contain a reference to the original dataset source in their filename. You must refer to the original licensing conditions for each video / dataset and filter out (remove) any videos you are not licensed to use (see table above).\n\nThe work of the **SPHAR Dataset** (aggregation, converting and cropping scripts) - but not the videos itself - are released under the **GNU GPL v3** license (contact me for further licensing options).\n\nSee the [LICENSE](LICENSE) file for more details.\n\n## Citation\nPlease note that none of the supplied videos have been recorded by myself.\n**Please attribute the original authors wherever possible.**\n\nIf you want to cite the work of the **SPHAR Dataset** (aggregation, converting and cropping), please link to this GitHub page. You can use the following BibTex entry:\n\n```bib\n@article{sphar-dataset,\n  title={SPHAR: Surveillance Perspective Human Action Recognition Dataset},\n  author={Alexander Melde},\n  year={2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  url = {https://github.com/AlexanderMelde/SPHAR-Dataset},\n  version = {\\UrlFont\\href{https://github.com/AlexanderMelde/SPHAR-Dataset/commit/40c1b9e}{40c1b9e}},\n  urldate={2020-07-18}\n}\n```\n(replace urldate with your own day of retrieval)\n\n[![SPHAR Surveillance Perspective Human Action Recognition Logo](docs/SPHAR.svg)](https://alexandermelde.github.io/SPHAR-Dataset/)</small>\n",
    "topics": [],
    "language": null
  },
  {
    "name": "Transformer_Implementation",
    "readme": "Transformer from Scratch with PyTorch\n=====================================\n\nThis project demonstrates how to build the foundational Transformer model‚Äîoriginally introduced in the paper _\"Attention is All You Need\"_‚Äîentirely from scratch using PyTorch. It walks through core components such as self-attention, positional encoding, and the encoder-decoder architecture, all with a clear, modular structure.\n\nOverview\n--------\n\nThe tutorial provides a hands-on approach to understanding and implementing a Transformer model, without relying on pre-built libraries like Hugging Face. It's perfect for those interested in learning the internal mechanics of modern sequence-to-sequence models, especially in natural language processing.\n\nKey Features\n------------\n\n*   **Self-Attention and Multi-Head Attention:** Learn how attention mechanisms allow the model to focus on different parts of the input sequence.\n    \n*   **Positional Encoding:** Understand how the model incorporates sequence order without recurrence or convolution.\n    \n*   **Encoder and Decoder Blocks:** See how layers are built using residual connections, layer normalization, and feed-forward networks.\n    \n*   **Masking and Padding:** Explore the importance of masking to prevent the model from attending to future tokens or padded positions.\n    \n*   **Training Loop:** Includes a basic training example on synthetic data for demonstration.\n    \n*   **Evaluation:** Basic validation logic is shown to measure the model‚Äôs learning progress.\n    \n\nWhat You Will Learn\n-------------------\n\n*   The inner workings of transformer models.\n    \n*   How to manually construct each module from the ground up.\n    \n*   How to stack layers to build a full encoder-decoder architecture.\n    \n*   How to train a transformer using cross-entropy loss and Adam optimizer.\n    \n*   How to evaluate model performance and prepare for downstream NLP tasks.\n    \n\nRequirements\n------------\n\nThis project uses PyTorch and standard Python libraries. You will need a basic Python environment with PyTorch installed.\n\nNotes\n-----\n\n*   The tutorial uses synthetic data to explain the model structure and training procedure. It is not meant to achieve state-of-the-art accuracy but to help you understand how transformers work under the hood.\n    \n*   For real-world applications, you should integrate tokenization, batching, learning rate scheduling, and use appropriate datasets.\n    \n\nFurther Resources\n-----------------\n\n*   The original paper: _‚ÄúAttention is All You Need‚Äù_\n    \n*   PyTorch documentation for deeper understanding of neural network modules\n    \n*   Hugging Face Transformers library for production-grade models\n    \n*   DataCamp‚Äôs advanced tutorials for practical NLP implementation\n    \n\nSummary\n-------\n\nThis tutorial is ideal for researchers, students, and developers who want to:\n\n*   Demystify the Transformer architecture\n    \n*   Learn how to implement it without external abstraction layers\n    \n*   Build a strong foundation for advanced work in NLP, machine translation, and large language models\n    \n\nFeel free to extend the project by:\n\n*   Integrating real datasets\n    \n*   Adding greedy or beam search decoding\n    \n*   Adapting the architecture for specific NLP tasks\n    \n\nLet me know if you want a markdown (.md) version or need this customized for GitHub!\n",
    "topics": [],
    "language": "Jupyter Notebook"
  },
  {
    "name": "Uber-Ride-Predictions--A-Data-Analysis-Project",
    "readme": "# Uber Ride Prediction - Model Comparison\n\nThis project aims to predict various aspects of Uber rides using **Logistic Regression** and **Random Forest** models. The main objective is to compare the performance of these models using both **test accuracy** and **cross-validation accuracy**.\n\n## Project Overview\n\nIn this project, two machine learning models (Logistic Regression and Random Forest) are applied to a dataset that contains information about Uber rides, including features such as:\n\n- **Start Location**\n- **End Location**\n- **Start Time**\n- **End Time**\n- **Distance traveled**\n- **Purpose of the ride**\n\nThe models are evaluated to predict various ride categories (e.g., **commute**, **meal/entertainment**, **customer visit**) and determine the probability of **high demand (surge pricing)**.\n\nThe key comparison metrics include:\n- **Test Accuracy**: The accuracy of the models on unseen data (test set).\n- **Cross-Validation Accuracy**: The accuracy achieved by the models on multiple folds of the dataset (cross-validation).\n\n## Features\n\n- **Logistic Regression**: A linear model used for binary classification.\n- **Random Forest**: An ensemble learning method used for both classification and regression tasks.\n\n## Dataset\n\nThe dataset used in this project contains the following columns:\n- **START_DATE**: The date the ride started.\n- **END_DATE**: The date the ride ended.\n- **START_LOCATION**: The starting point of the ride.\n- **END_LOCATION**: The destination point of the ride.\n- **MILES**: The distance traveled during the ride.\n- **PURPOSE**: The purpose of the ride (e.g., meeting, commute, etc.).\n- **CATEGORY**: The category of the ride (e.g., personal, business, etc.).\n- **START_TIME**: The starting hour and minute of the ride.\n- **END_TIME**: The ending hour and minute of the ride.\n\n## Project Workflow\n\n1. **Data Preprocessing**:\n   - Handle missing values.\n   - Encode categorical features (e.g., day of the week, start and end locations).\n   - Feature engineering for time-based features such as start and end hours, weekdays, and weekends.\n\n2. **Model Training**:\n   - Train **Logistic Regression** and **Random Forest** models using the training data.\n   - Perform **cross-validation** to evaluate model performance.\n   - Calculate **test accuracy** by evaluating the models on the test set.\n\n3. **Model Comparison**:\n   - Compare the accuracy of **Logistic Regression** and **Random Forest** based on:\n     - Test Accuracy\n     - Cross-Validation Accuracy\n\n4. **Visualization**:\n   - Create a **bar chart** comparing the test and cross-validation accuracy of both models for easy comparison.\n\n## Requirements\n\n- Python 3.x\n- Libraries:\n  - **NumPy**\n  - **Pandas**\n  - **Matplotlib**\n  - **Scikit-learn**\n\nInstall the necessary libraries using `pip`:\n\n```bash\npip install numpy pandas matplotlib scikit-learn\n\n\n---\n\nThis Markdown format includes all relevant project details, instructions, and usage guidelines. You can copy and paste this directly into your `README.md` file for your project. Let me know if you need any further adjustments!\n",
    "topics": [],
    "language": "Jupyter Notebook"
  },
  {
    "name": "us107",
    "readme": "# Hey there! fellow coders: <img src=\"https://github.com/us107/us107/assets/115691766/d1519347-3a3e-402a-b740-53132ecdbee9\" width=\"100\">\n\nTrisha Sharma here, diving into B.Tech for Data Science and Engineering at Manipal University Jaipur. Daytime coder, nighttime tech explorer, and a design lover 24/7! Let's create tech and design magic together! \n\nOh, and heads up, I'm not just a student ‚Äì I'm also a proud card-carrying member of the Jujutsu Kaisen (JJK) fandom and a certified Brooklyn Nine-Nine (B99) enthusiast! Let the fun and fandom begin! üéâ\n\n<img src=\"https://github.com/us107/us107/assets/115691766/fbe0ffb7-cdc4-47f5-99aa-71181e70f2c9\" width=\"600\">\n\n## üåê Socials:\n[![Discord](https://img.shields.io/badge/Discord-%237289DA.svg?logo=discord&logoColor=white)](https://discord.gg/ts299837) [![LinkedIn](https://img.shields.io/badge/LinkedIn-%230077B5.svg?logo=linkedin&logoColor=white)](https://linkedin.com/in/trisha-sharma-871544251) \n\n# üíª Tech Stack:\n![C](https://img.shields.io/badge/c-%2300599C.svg?style=plastic&logo=c&logoColor=white) ![MySQL](https://img.shields.io/badge/mysql-%2300000f.svg?style=plastic&logo=mysql&logoColor=white) ![MongoDB](https://img.shields.io/badge/MongoDB-%234ea94b.svg?style=plastic&logo=mongodb&logoColor=white)  ![Figma](https://img.shields.io/badge/figma-%23F24E1E.svg?style=plastic&logo=figma&logoColor=white) ![Dribbble](https://img.shields.io/badge/Dribbble-EA4C89?style=plastic&logo=dribbble&logoColor=white) ![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=plastic&logo=numpy&logoColor=white) ![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=plastic&logo=pandas&logoColor=white) ![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=plastic&logo=Matplotlib&logoColor=black) ![Scipy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=plastic&logo=scipy&logoColor=%white) ![scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=plastic&logo=scikit-learn&logoColor=white) ![Python](https://img.shields.io/badge/python-3670A0?style=plastic&logo=python&logoColor=ffdd54) ![Java](https://img.shields.io/badge/java-%23ED8B00.svg?style=plastic&logo=openjdk&logoColor=white) ![PowerShell](https://img.shields.io/badge/PowerShell-%235391FE.svg?style=plastic&logo=powershell&logoColor=white)\n\n\n### For me coding be like this:\n![240885348-491e3e44-11a0-487a-b07b-717f677bbe4a](https://github.com/us107/us107/assets/115691766/27693fdb-f9bb-45b6-9236-2fc1582c0c13)\n\n",
    "topics": [],
    "language": null
  },
  {
    "name": "UserProfile",
    "readme": "# UserProfile RiverDale\n",
    "topics": [],
    "language": null
  },
  {
    "name": "Youtube_trend_analysis",
    "readme": "# Youtube_trend_analysis",
    "topics": [],
    "language": "Jupyter Notebook"
  }
]